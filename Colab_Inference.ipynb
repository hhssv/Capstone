{"cells":[{"cell_type":"markdown","metadata":{"id":"o3fnLh3GzD8v"},"source":["## **Pedestrian Estimation**\n","\n","\n","> Goggle Drive Mount\n","\n","> Preparation\n","+ clone git\n","+ install the requirements\n","+ prepare the dependencies and the functions\n","+ set the config\n","\n","> Video Inference\n","+ set the video path\n","\n","> Image Inference\n","+ set the image path"]},{"cell_type":"markdown","metadata":{"id":"ZypET6s7zNii"},"source":["# 1. Goggle Drive Mount"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":360},"executionInfo":{"elapsed":13900,"status":"error","timestamp":1683536252990,"user":{"displayName":"이창형 | 전자공학부 | 한양대(ERICA) ­","userId":"17310207240374869556"},"user_tz":-540},"id":"ihCS8MbFxAwR","outputId":"add0b0b3-106f-4a0c-d2d3-ae3f8de07e4d"},"outputs":[{"output_type":"error","ename":"MessageError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-27eafb0abf78>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 드라이브 마운트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["# 드라이브 마운트\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"8dt-L1gdzSAH"},"source":["# 2. Preparation "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nhnSlz_rA3kQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684511115918,"user_tz":-540,"elapsed":23084,"user":{"displayName":"hhssvv","userId":"16444816965703080007"}},"outputId":"0bdabbe4-0409-40ab-97e7-091571f09eb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Capstone'...\n","remote: Enumerating objects: 374, done.\u001b[K\n","remote: Counting objects: 100% (223/223), done.\u001b[K\n","remote: Compressing objects: 100% (188/188), done.\u001b[K\n","remote: Total 374 (delta 37), reused 214 (delta 29), pack-reused 151\u001b[K\n","Receiving objects: 100% (374/374), 266.62 MiB | 16.73 MiB/s, done.\n","Resolving deltas: 100% (74/74), done.\n","Updating files: 100% (276/276), done.\n"]}],"source":["!git clone https://github.com/hhssv/Capstone.git "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_jMzFolA4M6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684511126738,"user_tz":-540,"elapsed":10834,"user":{"displayName":"hhssvv","userId":"16444816965703080007"}},"outputId":"1615f016-ade6-4d7b-fa90-30d32c32bb36"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Capstone\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.7.1)\n","Requirement already satisfied: numpy<1.24.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.22.4)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.7.0.72)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (8.4.0)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (6.0)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.27.1)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.10.1)\n","Requirement already satisfied: torch!=1.12.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.0.1+cu118)\n","Requirement already satisfied: torchvision!=0.13.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.15.2+cu118)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (4.65.0)\n","Requirement already satisfied: protobuf<4.21.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (3.20.3)\n","Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (2.12.2)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (1.5.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (0.12.2)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 34)) (7.34.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 35)) (5.9.5)\n","Collecting thop (from -r requirements.txt (line 36))\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Collecting filterpy (from -r requirements.txt (line 42))\n","  Downloading filterpy-1.4.5.zip (177 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 43)) (0.19.3)\n","Collecting timm==0.6.7 (from -r requirements.txt (line 46))\n","  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting yacs (from -r requirements.txt (line 49))\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (23.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (16.0.5)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.54.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.4.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.7.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (2.3.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.40.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 21)) (2022.7.1)\n","Collecting jedi>=0.16 (from ipython->-r requirements.txt (line 34))\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (3.0.38)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (2.14.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (4.8.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 43)) (2.25.1)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 43)) (2023.4.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 43)) (1.4.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.3.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.16.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.3.1)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->-r requirements.txt (line 34)) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->-r requirements.txt (line 34)) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirements.txt (line 34)) (0.2.6)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->-r requirements.txt (line 17)) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (1.3.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.2.2)\n","Building wheels for collected packages: filterpy\n","  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110459 sha256=a62237c808054e103b616f666e8675663866254ddbe5d9ffb24794d89cfe570b\n","  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n","Successfully built filterpy\n","Installing collected packages: yacs, jedi, filterpy, timm, thop\n","Successfully installed filterpy-1.4.5 jedi-0.18.2 thop-0.1.1.post2209072238 timm-0.6.7 yacs-0.1.8\n"]}],"source":["%cd /content/Capstone/\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1GqXX0mcMVJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684511156476,"user_tz":-540,"elapsed":29743,"user":{"displayName":"hhssvv","userId":"16444816965703080007"}},"outputId":"e845684e-2316-4231-89e0-090ae9381b64"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Capstone/trained_models\n","Downloading...\n","From: https://drive.google.com/uc?id=1q6a0RJuD54Gq8txw6TKPRRnC5xnSucLR\n","To: /content/Capstone/trained_models/BSUVNet-emptyBG-recentBG.mdl\n","100% 122M/122M [00:03<00:00, 39.7MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1ISzZyLDzuRuMnNmrZ3QVJCVeT_3eltDK\n","To: /content/Capstone/trained_models/BSUVNet-emptyBG-recentBG-FPM.mdl\n","100% 122M/122M [00:03<00:00, 34.8MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=12y2PMK8Ne7G27CI5Vx6hC2qkoF5cJo5I\n","To: /content/Capstone/trained_models/BSUV-Net-2.0.mdl\n","100% 122M/122M [00:07<00:00, 17.2MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=12y944z5yPePy2RFPu8V4VQr9IfBhLjUM\n","To: /content/Capstone/trained_models/Fast-BSUV-Net-2.0.mdl\n","100% 122M/122M [00:06<00:00, 20.2MB/s]\n"]}],"source":["#trained_models 폴더에 BSUV-Net-2.0 이동 \n","%cd /content/Capstone/trained_models/\n","!gdown https://drive.google.com/uc?id=1q6a0RJuD54Gq8txw6TKPRRnC5xnSucLR\n","!gdown https://drive.google.com/uc?id=1ISzZyLDzuRuMnNmrZ3QVJCVeT_3eltDK\n","!gdown https://drive.google.com/uc?id=12y2PMK8Ne7G27CI5Vx6hC2qkoF5cJo5I\n","!gdown https://drive.google.com/uc?id=12y944z5yPePy2RFPu8V4VQr9IfBhLjUM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sp_wqYRoc2Bu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684511177794,"user_tz":-540,"elapsed":21334,"user":{"displayName":"hhssvv","userId":"16444816965703080007"}},"outputId":"ea80df6d-d3de-465c-dde0-70f294070e76"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Capstone/utils/segmentation/hrnet_v2\n","Downloading...\n","From: https://drive.google.com/uc?id=1wf2BU76UnUyNS-8DTPc7gODm-TLENPBw\n","To: /content/Capstone/utils/segmentation/hrnet_v2/encoder_epoch_30.pth\n","100% 263M/263M [00:13<00:00, 19.3MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1WZr6cuz5rVBlhaaZFR9oWy1mzO4K4g9D\n","To: /content/Capstone/utils/segmentation/hrnet_v2/decoder_epoch_30.pth\n","100% 4.78M/4.78M [00:00<00:00, 18.6MB/s]\n"]}],"source":["#utils/segmentation/brnet_v2 폴더에 BSUV-Net-2.0 이동 \n","%cd /content/Capstone/utils/segmentation/hrnet_v2/\n","!gdown https://drive.google.com/uc?id=1wf2BU76UnUyNS-8DTPc7gODm-TLENPBw\n","!gdown https://drive.google.com/uc?id=1WZr6cuz5rVBlhaaZFR9oWy1mzO4K4g9D"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4gzsbb2HQnSk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684511274576,"user_tz":-540,"elapsed":96795,"user":{"displayName":"hhssvv","userId":"16444816965703080007"}},"outputId":"24e4d331-2072-404a-f858-5fce06b9f8c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Capstone\n","/usr/local/lib/python3.10/dist-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or help(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use help(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n","  warnings.warn(\n","Downloading: \"https://github.com/intel-isl/MiDaS/zipball/master\" to /root/.cache/torch/hub/master.zip\n","******************** Testing zoedepth ********************\n","Config:\n","{'attractor_alpha': 1000,\n"," 'attractor_gamma': 2,\n"," 'attractor_kind': 'mean',\n"," 'attractor_type': 'inv',\n"," 'aug': True,\n"," 'bin_centers_type': 'softplus',\n"," 'bin_embedding_dim': 128,\n"," 'clip_grad': 0.1,\n"," 'dataset': 'nyu',\n"," 'distributed': True,\n"," 'force_keep_ar': True,\n"," 'gpu': None,\n"," 'img_size': [384, 512],\n"," 'inverse_midas': False,\n"," 'log_images_every': 0.1,\n"," 'max_temp': 50.0,\n"," 'max_translation': 100,\n"," 'memory_efficient': True,\n"," 'midas_model_type': 'DPT_BEiT_L_384',\n"," 'min_temp': 0.0212,\n"," 'model': 'zoedepth',\n"," 'n_attractors': [16, 8, 4, 1],\n"," 'n_bins': 64,\n"," 'name': 'ZoeDepth',\n"," 'notes': '',\n"," 'output_distribution': 'logbinomial',\n"," 'prefetch': False,\n"," 'pretrained_resource': 'url::https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_N.pt',\n"," 'print_losses': False,\n"," 'project': 'ZoeDepth',\n"," 'random_crop': False,\n"," 'random_translate': False,\n"," 'root': '.',\n"," 'save_dir': '/root/shortcuts/monodepth3_checkpoints',\n"," 'shared_dict': None,\n"," 'tags': '',\n"," 'train_midas': False,\n"," 'translate_prob': 0.2,\n"," 'uid': None,\n"," 'use_amp': False,\n"," 'use_pretrained_midas': False,\n"," 'use_shared_dict': False,\n"," 'validate_every': 0.25,\n"," 'version_name': 'v1',\n"," 'workers': 16}\n","img_size [384, 512]\n","Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n","/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","Params passed to Resize transform:\n","\twidth:  512\n","\theight:  384\n","\tresize_target:  True\n","\tkeep_aspect_ratio:  True\n","\tensure_multiple_of:  32\n","\tresize_method:  minimal\n","Using pretrained resource url::https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_N.pt\n","Downloading: \"https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_N.pt\" to /root/.cache/torch/hub/checkpoints/ZoeD_M12_N.pt\n","100% 1.34G/1.34G [01:01<00:00, 23.6MB/s]\n","Loaded successfully\n","--------------------Testing on a random input--------------------\n","metric_depth torch.Size([1, 1, 384, 512])\n","\n","\n","\n","-------------------- Testing on an indoor scene from url --------------------\n","X.shape torch.Size([1, 3, 193, 260])\n","predicting\n","output.shape torch.Size([1, 1, 193, 260])\n","saved pred.png\n"]}],"source":["%cd /content/Capstone/\n","!python sanity.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l0hrR7l2zKYB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684511279392,"user_tz":-540,"elapsed":4822,"user":{"displayName":"hhssvv","userId":"16444816965703080007"}},"outputId":"a89ec29f-28b2-42b0-cc94-fe08965a6d9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Capstone\n"]}],"source":["# import dependencies\n","%cd /content/Capstone/\n","import argparse\n","import math\n","import time\n","from pathlib import Path\n","import cv2\n","import torch\n","import numpy as np\n","import torch.backends.cudnn as cudnn\n","from numpy import random\n","from tqdm.auto import tqdm\n","\n","# SORT Import\n","from sort import * # sort.py yolov7 폴더, sort.py 내부 matplotlib.use('TKAgg') 지우기\n","\n","from models.experimental import attempt_load\n","from utils.datasets import LoadStreams, LoadImages\n","from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n","    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n","from utils.plots import plot_one_box\n","from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n","\n","# ZoeDepth Import\n","from torchvision import transforms\n","from zoedepth.utils.misc import save_raw_16bit, colorize\n","from PIL import Image, ImageTk\n","import matplotlib.pyplot as plt\n","\n","# BSUVNet Import\n","import sys\n","import configs.infer_config_autoBG as cfg\n","from utils.data_loader import videoLoader\n","\n","# GUI Import\n","import tkinter as tk\n","import tkinter.ttk as ttk\n","import tkinter.font as font\n","\n","def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n","    # Resize and pad image while meeting stride-multiple constraints\n","    shape = img.shape[:2]  # current shape [height, width]\n","    if isinstance(new_shape, int):\n","        new_shape = (new_shape, new_shape)\n","\n","    # Scale ratio (new / old)\n","    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n","    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n","        r = min(r, 1.0)\n","\n","    # Compute padding\n","    ratio = r, r  # width, height ratios\n","    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n","    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n","    if auto:  # minimum rectangle\n","        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n","    elif scaleFill:  # stretch\n","        dw, dh = 0.0, 0.0\n","        new_unpad = (new_shape[1], new_shape[0])\n","        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n","\n","    dw /= 2  # divide padding into 2 sides\n","    dh /= 2\n","\n","    if shape[::-1] != new_unpad:  # resize\n","        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n","    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n","    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n","    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n","    return img, ratio, (dw, dh)\n","\n","def draw_boxes(img, bbox, identities=None, categories=None, confidences = None, names=None, colors = None):\n","    for i, box in enumerate(bbox):\n","        x1, y1, x2, y2 = [int(i) for i in box]\n","        tl = opt['thickness'] or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n","\n","        cat = int(categories[i]) if categories is not None else 0\n","        id = int(identities[i]) if identities is not None else 0\n","        # conf = confidences[i] if confidences is not None else 0\n","\n","        color = colors[cat]\n","        \n","        if not opt['nobbox']:\n","            cv2.rectangle(img, (x1, y1), (x2, y2), color, tl)\n","\n","        if not opt['nolabel']:\n","            label = str(id) + \":\"+ names[cat] if identities is not None else  f'{names[cat]} {confidences[i]:.2f}'\n","            tf = max(tl - 1, 1)  # font thickness\n","            t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n","            c2 = x1 + t_size[0], y1 - t_size[1] - 3\n","            cv2.rectangle(img, (x1, y1), c2, color, -1, cv2.LINE_AA)  # filled\n","            cv2.putText(img, label, (x1, y1 - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n","\n","    return img \n","\n","def crop_img(img, bbox, type='edgepoint'):\n","  \"\"\"\n","    img(numpy.ndarray) : shape = [H, W, C] \n","    bbox(numpy.ndarray) : shape = 'type' midpoint : [X, Y, W, H] or edgepoint : [X1, Y1, X2, Y2]\n","\n","  \"\"\"\n","  if isinstance(img, torch.Tensor):\n","        img = img.detach().cpu().numpy()\n","  if isinstance(bbox, torch.Tensor):\n","        bbox = bbox.detach().cpu().numpy()\n","\n","  if type == 'edgepoint':\n","    x_left = int(bbox[0])\n","    y_left = int(bbox[1])\n","    x_right = int(bbox[2])\n","    y_right = int(bbox[3])\n","\n","    cropped_img = img[y_left:y_right, x_left:x_right, :]\n","\n","  else: # for [X, Y, H, W]\n","    x_left = int(bbox[0] - bbox[2] / 2)\n","    y_left = int(bbox[1] - bbox[3] / 2)\n","    x_right = int(bbox[0] + bbox[2] / 2)\n","    y_right = int(bbox[1] + bbox[3] / 2)\n","    cropped_img = img[y_left:y_right, x_left:x_right, :]\n","  \n","  return cropped_img\n","\n","def get_grad(track, img0, thickness, sep=1, rate=5, gap=5, line_len=50): # sep : 간격, rate : accumulation rate\n","    epsil = 1e-7\n","    grad = 0\n","    direc = None\n","    pix_len = 0\n","\n","    if len(track.centroidarr) - sep * rate - gap - 1 >= 0:\n","        track.centroidarr = track.centroidarr[-sep * rate - gap - 1:]\n","        for i, _ in enumerate(track.centroidarr):\n","            if i < rate: # 0 : X, 1 : Y\n","                if (track.centroidarr[(i + 1) * sep + gap - 1][1] - track.centroidarr[i * sep][1]) == 0 or (track.centroidarr[(i + 1) * sep + gap - 1][0] - track.centroidarr[i * sep][0]) == 0:\n","                    grad += 0\n","                else:\n","                    grad += (track.centroidarr[(i + 1) * sep + gap - 1][1] - track.centroidarr[i * sep][1] + epsil) / (track.centroidarr[(i + 1) * sep + gap - 1][0] - track.centroidarr[i * sep][0] + epsil)\n","                grad /= rate\n","                theta = np.arctan(grad)\n","                if (track.centroidarr[(i + 1) * sep + gap - 1][1] - track.centroidarr[i * sep][1]) == 0 and (track.centroidarr[(i + 1) * sep + gap - 1][0] - track.centroidarr[i * sep][0]) == 0:\n","                    ptx = int(track.centroidarr[-1][0])\n","                    pty = int(track.centroidarr[-1][1])\n","                    direc = 2 # None\n","                else:\n","                    if (track.centroidarr[(i + 1) * sep + gap - 1][0] - track.centroidarr[i * sep][0]) > 0 :\n","                        ptx = int(track.centroidarr[-1][0]) + line_len * np.cos(theta)\n","                        pty = int(track.centroidarr[-1][1]) + line_len * np.sin(theta)\n","                        direc = 1 # right\n","                    else:\n","                        ptx = int(track.centroidarr[-1][0]) - line_len * np.cos(theta)\n","                        pty = int(track.centroidarr[-1][1]) + line_len * np.sin(theta)\n","                        direc = 0 # left\n","        pix_len = (int(((track.centroidarr[-2][0] - track.centroidarr[-1][0]) ** 2 + (track.centroidarr[-2][1] - track.centroidarr[-1][1]) ** 2) ** 0.5) +\n","        int(((track.centroidarr[-3][0] - track.centroidarr[-2][0]) ** 2 + (track.centroidarr[-3][1] - track.centroidarr[-2][1]) ** 2) ** 0.5) + \n","        int(((track.centroidarr[-4][0] - track.centroidarr[-3][0]) ** 2 + (track.centroidarr[-4][1] - track.centroidarr[-3][1]) ** 2) ** 0.5) +\n","        int(((track.centroidarr[-5][0] - track.centroidarr[-4][0]) ** 2 + (track.centroidarr[-5][1] - track.centroidarr[-4][1]) ** 2) ** 0.5) + \n","        int(((track.centroidarr[-6][0] - track.centroidarr[-5][0]) ** 2 + (track.centroidarr[-6][1] - track.centroidarr[-5][1]) ** 2) ** 0.5)) / 5\n","            #if i == rate - 1:\n","                #tl = opt['thickness'] or round(0.002 * (img0.shape[0] + img0.shape[1]) / 2) + 1  # line/font thickness\n","                #tf = max(tl - 1, 1)  # font thickness\n","                #cv2.line(img0, (int(track.centroidarr[-1][0]), int(track.centroidarr[-1][1])), (int(ptx), int(pty)), (0, 0, 0), thickness=thickness) # Direction Plot\n","                #cv2.putText(img0, '{:.2f}'.format(grad * (180 / 3.14)), (int(track.centroidarr[-1][0]), int(track.centroidarr[-1][1])),  0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n","    return grad, direc, pix_len\n","\n","def intersect_over_union(bbox1, bbox2):\n","    \"\"\"\n","      Compute the iou between two boxes (x1, y1, x2, y2)\n","    \"\"\"\n","    x1 = max(bbox1[0], bbox2[0])\n","    y1 = max(bbox1[1], bbox2[1])\n","    x2 = min(bbox1[2], bbox2[2])\n","    y2 = min(bbox1[3], bbox2[3])\n","    \n","    w = max(0, x2 - x1)\n","    h = max(0, y2 - y1)\n","\n","    intersection = w * h\n","    return intersection\n","\n","def transform_bbox_xywh_to_xyxy(bbox):\n","    \"\"\"\n","      Transform the shape of the bouding box [x, y, w, h] to [x, y, x, y]\n","      * cv2.boundingrect returns x1, y1, w, h ~ (x1, y1) -> top-left point\n","    \"\"\"\n","    x1 = int(bbox[0])\n","    y1 = int(bbox[1])\n","    x2 = int(bbox[0] + bbox[2])\n","    y2 = int(bbox[1] + bbox[3])\n","\n","    return [x1, y1, x2, y2]\n","\n","def compute_depth(img, bbox):\n","    \"\"\"\n","      Compute the depth values cropped with bbox\n","    \"\"\"\n","    result = []\n","    for i, box in enumerate(bbox):\n","      result.append(crop_img(img, box).mean())\n","    return result\n","\n","def find_contour_bbox(img,threshold=10, draw_bbox=True):\n","    \"\"\"\n","      Return the bounding box of the contours satisfying the threshold both of width and height.\n","\n","      return [M, 4] list. [x1, y1, x2, y2]\n","    \"\"\"\n","    result = []\n","    # Find contours and draw bounding boxes\n","    contours, hierarchy = cv2.findContours(fr[:, :, 0], cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    for cnt in contours: \n","      bb_x, bb_y, bb_w, bb_h = cv2.boundingRect(cnt)\n","      if bb_w > threshold and bb_h > threshold:\n","        if draw_bbox:\n","          cv2.rectangle(img, (bb_x, bb_y), (bb_x + bb_w, bb_y + bb_h), (0, 255, 0), 3)\n","        bbox = transform_bbox_xywh_to_xyxy([bb_x, bb_y, bb_w, bb_h])\n","        result.append(bbox)\n","    return result\n","\n","def judge_grad(grads, grad_threshold=40):\n","    \"\"\"\n","      grads : [N]\n","      grad_threshold : int, in degree\n","\n","      return [N] np.array, the pedestrian turns his/her direction, 0 - no, 1 - turn\n","    \"\"\"\n","    result = np.zeros(len(grads), dtype=np.int8)\n","    for i, grad in enumerate(grads):\n","      if grad > grad_threshold or grad < -grad_threshold:\n","        result[i] = 1\n","      else:\n","        result[i] = 0\n","    return result\n","\n","def judge_velocity(vel, vel_threshold=900):\n","    \"\"\"\n","      vel : [N]\n","      vel_threshold\n","\n","      return [N] list, 0 - no, 1 - fast(runner, cyclist, scooter etc)\n","    \"\"\"\n","    result = []\n","    for i, vel in enumerate(vel):\n","      if vel > vel_threshold:\n","        result.append(1)\n","      else:\n","        result.append(0)\n","    return result\n","\n","def judge_undefined_objects(bbox, mask=None, threshold=0.1):\n","    \"\"\"\n","      bbox : [N, 4]\n","      mask : [M, 4]\n","      iou_threshold : judge the iou between det and mask\n","      threshold : \n","      \n","      return the undefined objects' bbox [K, 4]\n","    \"\"\"\n","    result = []\n","    # Check undefined objects\n","    for i, box2 in enumerate(mask):\n","      # expand box size to remove noises in the overlapping cases\n","      for j, box1 in enumerate(bbox):\n","        w = int(box1[2] - box1[0])\n","        h = int(box1[3] - box1[1])\n","        iou = intersect_over_union([box1[0] - w * threshold, box1[1] - h * threshold, box1[2] + w * threshold, box1[3] + h * threshold], box2)\n","        if iou > 0.0:\n","          break\n","        if j == len(bbox) - 1: # unknown object\n","          result.append(box2)  \n","    return result\n","\n","def judge_depth(img, bbox, x_value=None, depth=None, depth_threshold=None, direc=None, vel_status=None, vel_threshold=1.4):\n","    \"\"\"\n","      judge the distance categories of the YOLO obejects\n","      bbox : [N, 4]\n","      x_values : [M], the x axis values to divide the image. ex) [500, 800]\n","      depth : [N]\n","      depth_threshold : [M + 1, 2], ex) [[100, 200], [10, 20], [30, 50], ...]\n","      direc : direction of pedestrians\n","      vel : velocity of objs\n","      vel_threshold : weight depth\n","\n","      return [N] : ex) ['warn', 'care', 'safe', ... , 'safe'], return the state of the pedestrian\n","    \"\"\"\n","    x_values_len = len(x_value)\n","    x_values = [0] + x_value + [img.shape[1]] # [0, 500, 800, 1280]\n","\n","    result = []\n","    # check bbox section\n","    if len(depth) > 0:\n","      for i, box in enumerate(bbox):\n","        ctx = (box[0] + box[2]) / 2\n","        if vel_status[i] == 1:\n","          depth[i] = depth[i] * vel_threshold\n","        for j in range(x_values_len + 1):\n","          if ctx <= x_values[j + 1] and ctx > x_values[j]:\n","            if (j == 2 and direc[i] == 1):\n","              result.append('safe_direc')\n","            else:\n","              if depth[i] > depth_threshold[j][1]:\n","                result.append('warn')\n","              else:\n","                if depth[i] > depth_threshold[j][0]:\n","                  result.append('care')\n","                else:\n","                  result.append('safe')\n","\n","    return result\n","\n","def identify_pedestrian_states(img, bbox, identities=None, \n","                               categories=None, names=None, grads=None, \n","                               depth=None, mask=None, direc=None, vel=None,\n","                               undefined_threshold=0.1, grad_threshold=40, vel_threshold=900,\n","                               x_values=[300, 800], depth_threshold=[[130, 140],[70, 100],[120, 160]], \n","                               img_sz=[1280, 720]):\n","    \"\"\"\n","      depth ~ Zoedepth results from cropped images, depth[N] \n","      mask ~ BSUVNet bounding box results ~ mask[N, 4]\n","\n","      color : Red, Yellow, Green, Blue\n","        Red : Warn\n","        Yellow : Care\n","        Green : Safe\n","        Blue : undefined class of objects\n","      \n","      now_frame\n","    \"\"\"\n","    car_bbox = None\n","    driver_bbox = None\n","    # Result\n","    result = [] # warn = 0, care = 1, safe = 2, unknown = 3\n","    logs = []\n","\n","    # color, BGR\n","    green = (0, 150, 0)\n","    red = (0, 0, 255)\n","    blue = (255, 0, 0)\n","    yellow = (0, 255, 255)\n","\n","    # undefined objects\n","    undefined_objects = judge_undefined_objects(bbox, mask, threshold=undefined_threshold)\n","\n","    # judgements\n","    grad_judge = judge_grad(grads, grad_threshold=grad_threshold) # [N]\n","    vel_judge = judge_velocity(vel=vel, vel_threshold=vel_threshold)\n","    depth_judge = judge_depth(img, bbox, x_value=x_values, depth=depth, \n","                              depth_threshold=depth_threshold, direc=direc, \n","                              vel_status=vel_judge, vel_threshold=1.4) # [N]\n","\n","    if len(undefined_objects) > 0 :\n","      for i, box in enumerate(undefined_objects):\n","        result.append([3, 'unknown'])\n","        x1, y1, x2, y2 = [int(i) for i in box]\n","        tl = opt['thickness'] or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n","        cv2.rectangle(img, (x1, y1), (x2, y2), blue, tl)\n","        \n","        label = f'unknown object'\n","        tf = max(tl - 1, 1)  # font thickness\n","        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n","        c2 = x1 + t_size[0], y1 - t_size[1] - 3\n","        cv2.rectangle(img, (x1, y1), c2, blue, -1, cv2.LINE_AA)  # filled\n","        cv2.putText(img, label, (x1, y1 - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n","\n","    # find driver\n","    for i, box in enumerate(bbox):\n","      x1, y1, x2, y2 = [int(i) for i in box]\n","      cat = int(categories[i]) if categories is not None else 0\n","      # ignore car and driver\n","      if names[cat] == 'car':\n","        w = int(x2 - x1)\n","        h = int(y2 - y1)\n","        car_bbox = [x1 + w * 0.2, y1 + h * 0.2, x2 - w * 0.2, y2 - h * 0.2] # threshold the car bbox\n","        continue\n","        \n","    # YOLO result\n","    for i, box in enumerate(bbox):\n","      x1, y1, x2, y2 = [int(i) for i in box]\n","      tl = opt['thickness'] or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n","      cat = int(categories[i]) if categories is not None else 0\n","      id = int(identities[i]) if identities is not None else 0\n","      # ignore car and driver\n","      if names[cat] == 'car':\n","        continue\n","      if car_bbox != None and intersect_over_union(car_bbox, box) > 0:\n","        car_bbox = None\n","        continue\n","\n","      # logging\n","      if len(grads) > 0 and len(depth) > 0 and len(vel) > 0:\n","        logs.append([str(id), grads[i], depth[i], vel[i], direc[i]])\n","\n","      # Colorize, Labeling\n","      if len(grad_judge) > 0:\n","        if grad_judge[i] == 1:\n","          result.append([0, 'grad', str(id)])\n","          label = f'{id} id warn'\n","          color = red\n","        else:\n","          if depth_judge[i] == 'warn':\n","            result.append([0, 'depth', str(id)])\n","            label = f'{id} id warn'\n","            color = red\n","          else:\n","            if depth_judge[i] == 'care':\n","              result.append([1, 'depth', str(id)])\n","              label = f'{id} id care'\n","              color = yellow\n","            else:\n","              if depth_judge[i] == 'safe':\n","                result.append([2, 'depth', str(id)])\n","                label = f'{id} id safe'\n","                color = green\n","              else:\n","                result.append([2, 'direction', str(id)])\n","                label = f'{id} id safe'\n","                color = green\n","      else:\n","        color = (0, 0, 0)\n","        label = f'None ??'\n","        result.append([5, None])\n","      cv2.rectangle(img, (x1, y1), (x2, y2), color, tl)\n","\n","      tf = max(tl - 1, 1)  # font thickness\n","      t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n","      c2 = x1 + t_size[0], y1 - t_size[1] - 3\n","      cv2.rectangle(img, (x1, y1), c2, color, -1, cv2.LINE_AA)  # filled\n","      cv2.putText(img, label, (x1, y1 - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n","    result.sort()\n","    # Display Sign\n","    if len(result) > 0:\n","      if result[0][0] == 0:\n","        now_status = 0\n","      else:\n","        if result[0][0] == 1:\n","          now_status = 1\n","        else:\n","          if result[0][0] == 2:\n","            now_status = 2\n","          else:\n","            now_status = 3\n","    else:\n","      now_status = None\n","    \n","    draw_dashboard(img, result, img_sz=img_sz)\n","    draw_infos(img, logs, img_sz=img_sz) # text all infos\n","    return img, now_status, logs\n","\n","def maintain_sign(img, now_status, pre_status, count, img_sz=[1280, 720]):\n","    \"\"\"\n","      warn - 3 sec\n","      care - 1 sec\n","      safe - 1 sec\n","      unknown - 1 sec\n","    \"\"\"\n","    # color, BGR\n","    green = (0, 150, 0)\n","    red = (0, 0, 255)\n","    blue = (255, 0, 0)\n","    yellow = (0, 255, 255)\n","    color = None\n","    if count == 0:\n","      if now_status == 0: # warn\n","        pre_status = 0\n","      else:\n","        if now_status == 1: # care\n","          pre_status = 1\n","        else:\n","          if now_status == 2: # safe\n","            pre_status = 2\n","          else:\n","            if now_status == 3: # unknown\n","              pre_status = 3 \n","            else : # None\n","              pre_status = None\n","    \n","    if pre_status == 0:\n","      count += 1\n","      color = red\n","      draw_sign(img, color, pre_status, img_sz=img_sz)\n","      if count == 90:\n","        count = 0\n","        pre_status = 3\n","    else:\n","      if pre_status == 1:\n","        count += 1\n","        color = yellow\n","        draw_sign(img, color, pre_status, img_sz=img_sz)\n","        if now_status == 0: # warn during care sign\n","          count = 0\n","          pre_status = 0\n","        if count == 30:\n","          count = 0\n","          pre_status = 2\n","      else:\n","        if pre_status == 2:\n","          count += 1\n","          color = green\n","          draw_sign(img, color, pre_status, img_sz=img_sz)\n","          if now_status == 0: # warn during safe sign\n","            count = 0\n","            pre_status = 0\n","          if now_status == 1: # care during safe sign\n","            count = 0\n","            pre_status = 1\n","          if count == 30:\n","            count = 0\n","            pre_status = 2\n","        else:\n","          if pre_status == 3:\n","            count += 1\n","            color = blue\n","            draw_sign(img, color, pre_status, img_sz=img_sz)\n","            if now_status == 0: # warn during unknown sign\n","              count = 0\n","              pre_status = 0\n","            if now_status == 1: # care during unknown sign\n","              count = 0\n","              pre_status = 1\n","            if now_status == 2: # safe during unknown sign\n","              count = 0\n","              pre_status = 2\n","            if count == 30:\n","              count = 0\n","              pre_status = 2\n","          else: # None\n","            count = 0\n","            pre_status = None\n","            draw_sign(img, (102, 102, 102), pre_status, img_sz=img_sz)\n","\n","    return pre_status, count\n","\n","def draw_sign(img, color, pre_status, img_sz=[1280, 720]):\n","    # color, BGR\n","    green = (0, 150, 0)\n","    red = (0, 0, 255)\n","    blue = (255, 0, 0)\n","    yellow = (0, 255, 255)\n","\n","    # for different image size\n","    ratio_w, ratio_h = img_sz[0] / 1280, img_sz[1] / 720\n","\n","    back_w, back_h = int(140 * ratio_w), int(360 * ratio_h) # 140 x 360\n","    back_pt1, back_pt2 = (int(img_sz[0] - back_w - 10 * ratio_w), int(10 * ratio_h)), (int(img_sz[0] - 10 * ratio_w), int(back_h + 10 * ratio_h))\n","\n","    radius = int(50 * ratio_w)\n","\n","    cc1 = (int((img_sz[0] - back_w / 2) - 10 * ratio_w), int(radius + (10 + 20) * ratio_h))\n","    cc2 = (int((img_sz[0] - back_w / 2) - 10 * ratio_w), int(radius * 3 + (10 * 2 + 20) * ratio_h))\n","    cc3 = (int((img_sz[0] - back_w / 2) - 10 * ratio_w), int(radius * 5 + (10 * 3 + 20) * ratio_h))\n","\n","    cv2.rectangle(img, back_pt1, back_pt2, (51, 51, 51), -1, cv2.LINE_AA) # background\n","    cv2.circle(img, cc1, radius, (102, 102, 102), -1) # first\n","    cv2.circle(img, cc2, radius, (102, 102, 102), -1) # second\n","    cv2.circle(img, cc3, radius, (102, 102, 102), -1) # third\n","\n","    if pre_status == 0:\n","      cv2.circle(img, cc1, radius, color, -1) # first\n","      cv2.circle(img, (int(cc1[0]-20 * ratio_w), int(cc1[1]-20 * ratio_h)), int(10 * ratio_w), (255, 255, 255), -1)\n","    if pre_status == 1:\n","      cv2.circle(img, cc2, radius, color, -1) # second\n","      cv2.circle(img, (int(cc2[0]-20 * ratio_w), int(cc2[1]-20 * ratio_h)), int(10 * ratio_w), (255, 255, 255), -1)\n","    if pre_status == 2:\n","      cv2.circle(img, cc3, radius, color, -1) # third\n","      cv2.circle(img, (int(cc3[0]-20 * ratio_w), int(cc3[1]-20 * ratio_h)), int(10 * ratio_w), (255, 255, 255), -1)\n","    if pre_status == 3:\n","      cv2.circle(img, cc1, radius, color, -1) # first\n","      cv2.circle(img, cc2, radius, color, -1) # second\n","      cv2.circle(img, cc3, radius, color, -1) # third   \n","      cv2.circle(img, (int(cc1[0]-20 * ratio_w), int(cc1[1]-20 * ratio_h)), int(10 * ratio_w), (255, 255, 255), -1)\n","      cv2.circle(img, (int(cc2[0]-20 * ratio_w), int(cc2[1]-20 * ratio_h)), int(10 * ratio_w), (255, 255, 255), -1)\n","      cv2.circle(img, (int(cc3[0]-20 * ratio_w), int(cc3[1]-20 * ratio_h)), int(10 * ratio_w), (255, 255, 255), -1)  \n","\n","def draw_dashboard(img, result, img_sz=[1280, 720], dash_sz=[400, 300]): # put the text about the result\n","    # color, BGR\n","    green = (0, 150, 0)\n","    red = (0, 0, 255)\n","    blue = (255, 0, 0)\n","    yellow = (0, 255, 255)\n","    result_text = None\n","    color = (102, 102, 102)\n","    # for different image size\n","    ratio_w, ratio_h = img_sz[0] / 1280, img_sz[1] / 720\n","    dashboard_sz = dash_sz.copy()\n","    dashboard_sz[0], dashboard_sz[1] = int(dash_sz[0] * ratio_w), int(dash_sz[1] * ratio_h)\n","    # PutText, result - [[warn, depth],[warn, grad], ...] - [N, N]\n","    font_size = 0.85 * ratio_w\n","    gap_size = int(30 * ratio_w)\n","    start_pt = [img_sz[0] - dashboard_sz[0] - 20 - 140, 10]\n","    # First we crop the sub-rect from the image\n","    sub_img = img[start_pt[1]:int(start_pt[1]+gap_size * len(result) + 50 * ratio_w), start_pt[0]:start_pt[0]+dashboard_sz[0]]\n","    white_rect = np.ones(sub_img.shape, dtype=np.uint8) * 255\n","    res = cv2.addWeighted(sub_img, 0.5, white_rect, 0.5, 1.0)\n","\n","    # Putting the image back to its position\n","    img[start_pt[1]:int(start_pt[1]+gap_size * len(result) + 50 * ratio_w), start_pt[0]:start_pt[0]+dashboard_sz[0]] = res\n","\n","    for i in range(len(result)):\n","      if result[i][0] == 0: # warn object\n","        result_text = f'{result[i][2]} id warn object, {result[i][1]}'\n","        color = red\n","      if result[i][0] == 1: # care object\n","        result_text = f'{result[i][2]} id care object, {result[i][1]}'\n","        color = yellow\n","      if result[i][0] == 2: # safe object\n","        result_text = f'{result[i][2]} id safe object, {result[i][1]}'\n","        color = green\n","      if result[i][0] == 3: # unknown object\n","        result_text = f'unknown object, {result[i][1]}'\n","        color = blue\n","      cv2.putText(img, result_text, (start_pt[0] + int(20 * ratio_w), gap_size * i + int(40 * ratio_h)), cv2.FONT_ITALIC, font_size, color, int(3 * ratio_w), lineType=cv2.LINE_AA)\n","    result_text = f'detect {len(result)} objects'\n","    color = green\n","    cv2.putText(img, result_text, (start_pt[0] + int(20 * ratio_w), gap_size * len(result) + int(40 * ratio_h)), cv2.FONT_ITALIC, font_size, color, int(3 * ratio_w), lineType=cv2.LINE_AA)\n","\n","def draw_infos(img, result, img_sz=[1280, 720], dash_sz=[600, 300]): # put the text about the result\n","    # color, BGR\n","    green = (0, 150, 0)\n","    red = (0, 0, 255)\n","    blue = (255, 0, 0)\n","    yellow = (0, 255, 255)\n","    result_text = None\n","    color = (102, 102, 102)\n","\n","    # for different image size\n","    ratio_w, ratio_h = img_sz[0] / 1280, img_sz[1] / 720\n","    dashboard_sz = dash_sz.copy()\n","    dashboard_sz[0], dashboard_sz[1] = int(dash_sz[0] * ratio_w), int(dash_sz[1] * ratio_h)\n","    font_size = 0.5 * ratio_w\n","    gap_size = int(20 * ratio_w)\n","    start_pt = [int(10 * ratio_w), int(10 * ratio_h)]\n","    # First we crop the sub-rect from the image\n","    sub_img = img[start_pt[1]:int(start_pt[1]+len(result) * gap_size + 30 * ratio_w), start_pt[0]:start_pt[0]+dashboard_sz[0]]\n","    white_rect = np.ones(sub_img.shape, dtype=np.uint8) * 255\n","    res = cv2.addWeighted(sub_img, 0.5, white_rect, 0.5, 1.0)\n","\n","    # Putting the image back to its position\n","    img[start_pt[1]:int(start_pt[1]+len(result) * gap_size + 30 * ratio_w), start_pt[0]:start_pt[0]+dashboard_sz[0]] = res\n","\n","    for i in range(len(result)):\n","      result_text = f'{result[i][0]} id | {result[i][1]:.2f} grad | {result[i][2]:.2f} depth | {result[i][3]:.2f} velocity | {result[i][4]} direction' # 0 left, 1 right\n","      cv2.putText(img, result_text, (start_pt[0] + int(20 * ratio_w), gap_size * i + int(40 * ratio_w)), cv2.FONT_ITALIC, font_size, (50, 50, 50), int(2 * ratio_w), lineType=cv2.LINE_AA)\n","\n","def logging_infos(logs, datas):\n","    \"\"\"\n","      match the id with the datas\n","      logs = [[id, grad, depth, vel, direc], ...]\n","      datas = []\n","      data : whole logs through frames = [[id, grad_list, depth_list, vel_list, direc_list], ...]\n","    \"\"\"\n","    for i, log in enumerate(logs):\n","      # check id\n","      if len(datas) == 0: # first\n","        datas = np.array([np.array([log[0], np.array([log[1]]), np.array([log[2]]), np.array([log[3]]), np.array([log[4]])])])\n","        continue\n","      for j in range(len(datas)):\n","        if int(log[0]) == int(datas[j, 0]): # correct id\n","          # add log to data\n","          datas[j, 1] = np.append(datas[j, 1], log[1])\n","          datas[j, 2] = np.append(datas[j, 2], log[2])\n","          datas[j, 3] = np.append(datas[j, 3], log[3])\n","          datas[j, 4] = np.append(datas[j, 4], log[4])\n","          break\n","        else: # incorrect id or new data\n","          # add log to data with new id\n","          if len(datas) == j + 1:\n","            datas = np.vstack((datas, [log[0], [log[1]], [log[2]], [log[3]], [log[4]]]))\n","    return datas\n","\n","def plot_infos(logs, figsize=(10, 5)):\n","    \"\"\"\n","      plot datas and save datas\n","    \"\"\"\n","    import matplotlib.pyplot as plt\n","    import seaborn as sns\n","    %matplotlib inline\n","\n","    plot_logs = logs.copy() # backup log\n","\n","    # plot logs\n","    for i, log in enumerate(plot_logs):\n","      # get id\n","      plot_id, grad_data, depth_data, vel_data, _ = log\n","      sns.set()\n","      plt.figure(figsize=figsize)\n","    \n","      plt.subplot(1, 3, 1)\n","      plt.plot(grad_data)\n","      plt.xlabel('Gradient')\n","      plt.ylabel('Degree')\n","\n","      plt.subplot(1, 3, 2)\n","      plt.title('{} Data'.format(plot_id))\n","      plt.plot(depth_data)\n","      plt.xlabel('Depth')\n","      plt.ylabel('value')\n","\n","      plt.subplot(1, 3, 3)\n","      plt.plot(vel_data)\n","      plt.xlabel('Velocity')\n","      plt.ylabel('value')\n","\n","      plt.savefig('{}_id.png'.format(plot_id))\n","      plt.show()\n","      \n","# initialze bounding box to empty\n","bbox = ''\n","count = 0 \n","\n","# Configuration\n","classes_to_filter = ['bicycle', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n","         'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n","         'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n","         'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n","         'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n","         'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n","         'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n","         'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n","         'hair drier', 'toothbrush' ] # COCO Dataset Class에서 원하지 않는 Class \n","\n","opt = {\n","    \"weights\": \"yolov7.pt\", # Path to weights file default weights are for nano model\n","    \"yaml\"   : \"data/coco.yaml\",\n","    \"img-size\": 640, # default image size\n","    \"conf-thres\": 0.25, # confidence threshold for inference.\n","    \"iou-thres\" : 0.45, # NMS IoU threshold for inference.\n","    \"device\" : 'cuda:0',  # device to run our model i.e. 0 or 0,1,2,3 or cpu\n","    \"classes\" : classes_to_filter,  # list of classes to filter or None\n","    \"track\" : True,\n","    \"show-track\" : True,\n","    \"unique-track-color\" : True,\n","    \"thickness\" : 2,\n","    \"nobbox\" : False,\n","    \"nolabel\" : False,\n","}"]},{"cell_type":"markdown","metadata":{"id":"NfLBA7lrzWqc"},"source":["# 3. Video Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OMQ8De3-yD4f","colab":{"base_uri":"https://localhost:8080/","height":418,"referenced_widgets":["0af71121b4b94d6598fc7f2d0b88e1df","90ad7fa6cbe8412d8044dd14918624cf","5e1527b55b75449aa295c5bf012e5c1c","ec57b6ac1d8e40b8a887c9a1d5bb57e0","0c638db7cae5467fbc723046cabad142","24ed9e35c2e8494783e921791460a966","f98e6a29a4ad4785b064bf0603365a6f","b4e9b5a90cd44e428359e0dde41093ac","dccf740b346741d695153bd7471e9e45","3b677b069ac442da917ae2871690bcc6","3f39621951274964bb0c75ea47a09dfb"]},"outputId":"2cc78c67-1759-44a4-c27d-13ab27804401","executionInfo":{"status":"ok","timestamp":1684508003001,"user_tz":-540,"elapsed":1042817,"user":{"displayName":"hhssvv","userId":"16444816965703080007"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["img_size [384, 512]\n"]},{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"]},{"output_type":"stream","name":"stdout","text":["Params passed to Resize transform:\n","\twidth:  512\n","\theight:  384\n","\tresize_target:  True\n","\tkeep_aspect_ratio:  True\n","\tensure_multiple_of:  32\n","\tresize_method:  minimal\n","Using pretrained resource url::https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_NK.pt\n","Loaded successfully\n","Empty background is completed\n","Fusing layers... \n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/348 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0af71121b4b94d6598fc7f2d0b88e1df"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["<ipython-input-34-d2d81461861d>:190: RuntimeWarning: Mean of empty slice.\n","  result.append(crop_img(img, box).mean())\n","/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n"]}],"source":["# 비디오 영상 처리\n","\n","# 비디오 영상 path 설정\n","video_path = './samples/9.Scooter.mp4' # 1280 * 720\n","\n","# Initializing video object\n","video = cv2.VideoCapture(video_path)\n","\n","# Video information\n","fps = video.get(cv2.CAP_PROP_FPS)\n","w = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n","h = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","nframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","# GradAcc Parameters\n","sep = 1 # Sep param \n","rate = 8 # Acc param\n","gap = 5 # Gap param\n","line_len = 50 # direction line length\n","\n","# Initializing SORT Class\n","sort_tracker = Sort(max_age=5, min_hits=2, iou_threshold=0.2) \n","\n","# ZoeDepth Model preparation\n","zoe = torch.hub.load(\".\", \"ZoeD_NK\", source=\"local\", pretrained=True)\n","zoe = zoe.to('cuda')\n","\n","# BSUVNet Initialization\n","# Start Video Loader\n","vid_loader = videoLoader(\n","    video_path, \n","    empty_bg=cfg.BSUVNet.emtpy_bg, \n","    empty_win_len=cfg.BSUVNet.empty_win_len,\n","    empty_bg_path=cfg.BSUVNet.empty_bg_path,\n","    recent_bg=cfg.BSUVNet.recent_bg,\n","    seg_network=cfg.BSUVNet.seg_network,\n","    transforms_pre=cfg.BSUVNet.transforms_pre,\n","    transforms_post=cfg.BSUVNet.transforms_post\n","    )\n","tensor_loader = iter(torch.utils.data.DataLoader(dataset=vid_loader, batch_size=1))\n","\n","# Load BSUV-Net\n","bsuvnet = torch.load(cfg.BSUVNet.model_path)\n","bsuvnet.cuda().eval()\n","\n","# Initialzing object for writing video output\n","output = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'DIVX'),fps , (w,h))\n","zoe_output = cv2.VideoWriter('zoe_output.mp4', cv2.VideoWriter_fourcc(*'DIVX'),fps , (w,h))\n","mask_output = cv2.VideoWriter('mask_output.mp4', cv2.VideoWriter_fourcc(*'DIVX'),fps , (w,h))\n","torch.cuda.empty_cache()\n","\n","# parameter\n","pre_status = None\n","dis_cnt = 0\n","mask_threshold = 10\n","\n","# logging\n","w_logs = np.array([])\n","#grad_history = []\n","#depth_history = []\n","#vel_history = []\n","# Initializing model and setting it for inference\n","with torch.no_grad():\n","  # YOLOv7 Preparation\n","  weights, imgsz = opt['weights'], opt['img-size']\n","  set_logging()\n","  device = select_device(opt['device'])\n","  half = device.type != 'cpu'\n","  model = attempt_load(weights, map_location=device)  # load FP32 model\n","  stride = int(model.stride.max())  # model stride\n","  imgsz = check_img_size(imgsz, s=stride)  # check img_size\n","  if half:\n","    model.half()\n","  names = model.module.names if hasattr(model, 'module') else model.names\n","  #colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n","  if device.type != 'cpu':\n","    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n","  classes = None\n","  if opt['classes']:\n","    classes = []\n","    for class_name in opt['classes']:\n","      classes.append(names.index(class_name))\n","  if classes:\n","    classes = [i for i in range(len(names)) if i not in classes]\n","\n","  # Start\n","  for j in tqdm(range(nframes)):\n","      data = next(tensor_loader)\n","      inp, img0 = data\n","      img0 = img0.squeeze().numpy()\n","      if True:\n","        # Zoedepth write results\n","        zoe_img = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n","        zoe_img = Image.fromarray(zoe_img)  # load image\n","        depth_pil = zoe.infer_pil(zoe_img, output_type=\"tensor\")  # tensor image\n","        colored_depth = colorize(depth_pil)[:, :, :3] # Gray scale\n","        \n","        # BSUVNet write results\n","        bgs_pred = bsuvnet(inp.cuda().float()).cpu().numpy()[0, 0, :, :]\n","        fr = np.ones((h, w, 3)) * 0.5\n","        for ch in range(3):\n","          fr[:, :, ch] = bgs_pred\n","        fr = (fr * 255).astype(np.uint8) # Result [720, 1280, 3], all channel have the same value, gray scale\n","\n","        # YOLO Prediction\n","        yolo_img = letterbox(img0, imgsz, stride=stride)[0]\n","        yolo_img = yolo_img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n","        yolo_img = np.ascontiguousarray(yolo_img)\n","        yolo_img = torch.from_numpy(yolo_img).to(device)\n","        yolo_img = yolo_img.half() if half else yolo_img.float()  # uint8 to fp16/32\n","        yolo_img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","        if yolo_img.ndimension() == 3:\n","          yolo_img = yolo_img.unsqueeze(0)\n","\n","        # YOLOv7 Inference\n","        #t1 = time_synchronized()\n","        pred = model(yolo_img, augment= False)[0]\n","        pred = non_max_suppression(pred, opt['conf-thres'], opt['iou-thres'], classes=classes, agnostic= False)\n","        #t2 = time_synchronized()\n","        \n","        #########\n","        grads = []\n","        depths = []\n","        direcs = []\n","        vel = []\n","        # YOLO Unpacking Predictions\n","        for i, det in enumerate(pred):\n","          #s = ''\n","          #s += '%gx%g ' % yolo_img.shape[2:]  # print string\n","          #gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n","          if len(det):\n","            det[:, :4] = scale_coords(yolo_img.shape[2:], det[:, :4], img0.shape).round()\n","\n","            #for c in det[:, -1].unique():\n","            #  n = (det[:, -1] == c).sum()  # detections per class\n","            #  s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n","\n","            dets_to_sort = np.empty((0,6))\n","\n","            # NOTE: We send in detected object class too\n","            for x1,y1,x2,y2,conf,detclass in det.cpu().detach().numpy():\n","              dets_to_sort = np.vstack((dets_to_sort, np.array([x1, y1, x2, y2, conf, detclass])))\n","            \n","            if opt['track']:\n","              tracked_dets = sort_tracker.update(dets_to_sort, opt['unique-track-color'])\n","              tracks = sort_tracker.getTrackers()\n","\n","              # draw boxes for visualization\n","              if len(tracked_dets)>0:\n","                  bbox_xyxy = tracked_dets[:,:4]\n","                  identities = tracked_dets[:, 8]\n","                  categories = tracked_dets[:, 4]\n","                  confidences = None\n","                  depths = compute_depth(colored_depth, bbox_xyxy)\n","                  mask_bbox = find_contour_bbox(fr, threshold=mask_threshold, draw_bbox=True)\n","                  for t, track in enumerate(reversed(tracks)):\n","                    grad = 0\n","                    direc = None\n","                    grad, direc, pix_len = get_grad(track, img0, opt['thickness'], sep, rate, gap, line_len) # draw line w/ cal grad, pts\n","                    grads.append(grad * (180 / 3.14))\n","                    direcs.append(direc)\n","                    vel.append(depths[t if t < len(tracked_dets) else 0] * pix_len)\n","                  #grad_history.append(grads)\n","                  #vel_history.append(vel)\n","                  #depth_history.append(depths)\n","            else:\n","              bbox_xyxy = dets_to_sort[:,:4]\n","              identities = None\n","              categories = dets_to_sort[:, 5]\n","              confidences = dets_to_sort[:, 4]\n","\n","            img0, now_status, logs = identify_pedestrian_states(img0, bbox_xyxy, identities=identities, \n","                                                          categories=categories, names=names, grads=grads,\n","                                                          depth=depths, mask=mask_bbox, direc=direcs, vel=vel,\n","                                                          undefined_threshold=0.1, grad_threshold=40, vel_threshold=900, \n","                                                          x_values=[300, 800], depth_threshold=[[130, 150],[70, 100],[120, 160]])\n","            pre_status, dis_cnt = maintain_sign(img0, now_status, pre_status, dis_cnt)\n","            w_logs = logging_infos(logs, datas=w_logs)\n","        #cv2.imwrite('output.png', img0)\n","        zoe_output.write(colored_depth)\n","        mask_output.write(fr)\n","        output.write(img0)\n","      else:\n","        break\n","    \n","output.release()\n","zoe_output.release()\n","mask_output.release()\n","video.release()"]},{"cell_type":"code","source":["print(w_logs.shape)"],"metadata":{"id":"skE6S88sUCtR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_infos(w_logs, figsize=(20, 5))"],"metadata":{"id":"dOYriOWIT4w4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Webcam Inference\n"],"metadata":{"id":"c9dIvRGOWZ6K"}},{"cell_type":"markdown","source":["Colab에서는 작동 X"],"metadata":{"id":"A7w4lSnRn0b0"}},{"cell_type":"code","source":["# Webcam\n","video = cv2.VideoCapture(0)\n","\n","# Video information\n","fps = int(video.get(cv2.CAP_PROP_FPS))\n","w = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n","h = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","#nframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","# GradAcc Parameters\n","sep = 1 # Sep param \n","rate = 8 # Acc param\n","gap = 5 # Gap param\n","line_len = 50 # direction line length\n","\n","# Initializing SORT Class\n","sort_tracker = Sort(max_age=5, min_hits=2, iou_threshold=0.2) \n","\n","# ZoeDepth Model preparation\n","zoe = torch.hub.load(\".\", \"ZoeD_NK\", source=\"local\", pretrained=True)\n","zoe = zoe.to('cuda')\n","\n","# BSUVNet Initialization\n","# Start Video Loader\n","vid_loader = videoLoader(\n","    0, \n","    empty_bg=cfg.BSUVNet.emtpy_bg, \n","    empty_win_len=cfg.BSUVNet.empty_win_len,\n","    empty_bg_path=cfg.BSUVNet.empty_bg_path,\n","    recent_bg=cfg.BSUVNet.recent_bg,\n","    seg_network=cfg.BSUVNet.seg_network,\n","    transforms_pre=cfg.BSUVNet.transforms_pre,\n","    transforms_post=cfg.BSUVNet.transforms_post\n","    )\n","tensor_loader = torch.utils.data.DataLoader(dataset=vid_loader, batch_size=1)\n","\n","# Load BSUV-Net\n","bsuvnet = torch.load(cfg.BSUVNet.model_path)\n","bsuvnet.cuda().eval()\n","\n","# Initialzing object for writing video output\n","output = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'DIVX'),fps , (w,h))\n","zoe_output = cv2.VideoWriter('zoe_output.mp4', cv2.VideoWriter_fourcc(*'DIVX'),fps , (w,h))\n","mask_output = cv2.VideoWriter('mask_output.mp4', cv2.VideoWriter_fourcc(*'DIVX'),fps , (w,h))\n","torch.cuda.empty_cache()\n","\n","# parameter\n","pre_status = None\n","dis_cnt = 0\n","mask_threshold = 10\n","\n","# logging\n","w_logs = np.array([])\n","#grad_history = []\n","#depth_history = []\n","#vel_history = []\n","# Initializing model and setting it for inference\n","with torch.no_grad():\n","  # YOLOv7 Preparation\n","  weights, imgsz = opt['weights'], opt['img-size']\n","  set_logging()\n","  device = select_device(opt['device'])\n","  half = device.type != 'cpu'\n","  model = attempt_load(weights, map_location=device)  # load FP32 model\n","  stride = int(model.stride.max())  # model stride\n","  imgsz = check_img_size(imgsz, s=stride)  # check img_size\n","  if half:\n","    model.half()\n","  names = model.module.names if hasattr(model, 'module') else model.names\n","  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n","  if device.type != 'cpu':\n","    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n","  classes = None\n","  if opt['classes']:\n","    classes = []\n","    for class_name in opt['classes']:\n","      classes.append(names.index(class_name))\n","  if classes:\n","    classes = [i for i in range(len(names)) if i not in classes]\n","\n","  # Start\n","  #for j in tqdm(range(nframes)):\n","  while video.isOpened():\n","    for data in tensor_loader:\n","      t1 = time_synchronized()\n","      inp, img0 = data\n","      img0 = img0.squeeze().numpy()\n","      # Zoedepth write results\n","      zoe_img = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB) \n","      zoe_img = Image.fromarray(zoe_img)  # transform cv2 to pil\n","      depth_pil = zoe.infer_pil(zoe_img, output_type=\"tensor\")  # tensor image\n","      colored_depth = colorize(depth_pil)[:, :, :3] # Gray scale\n","      # BSUVNet write results\n","      bgs_pred = bsuvnet(inp.cuda().float()).cpu().numpy()[0, 0, :, :]\n","      fr = np.ones((h, w, 3)) * 0.5\n","      for ch in range(3):\n","        fr[:, :, ch] = bgs_pred\n","      fr = (fr * 255).astype(np.uint8) # Result [720, 1280, 3], all channel have the same value, gray scale\n","      # YOLO Prediction\n","      yolo_img = letterbox(img0, imgsz, stride=stride)[0]\n","      yolo_img = yolo_img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n","      yolo_img = np.ascontiguousarray(yolo_img)\n","      yolo_img = torch.from_numpy(yolo_img).to(device)\n","      yolo_img = yolo_img.half() if half else yolo_img.float()  # uint8 to fp16/32\n","      yolo_img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","      if yolo_img.ndimension() == 3:\n","        yolo_img = yolo_img.unsqueeze(0)\n","\n","      # YOLOv7 Inference\n","      pred = model(yolo_img, augment= False)[0]\n","      pred = non_max_suppression(pred, opt['conf-thres'], opt['iou-thres'], classes= classes, agnostic= False)\n","        \n","      #########\n","      grads = []\n","      depths = []\n","      direcs = []\n","      vel = []\n","      # YOLO Unpacking Predictions\n","      for i, det in enumerate(pred):\n","        s = ''\n","        s += '%gx%g ' % yolo_img.shape[2:]  # print string\n","        gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n","        if len(det):\n","          det[:, :4] = scale_coords(yolo_img.shape[2:], det[:, :4], img0.shape).round()\n","\n","          for c in det[:, -1].unique():\n","            n = (det[:, -1] == c).sum()  # detections per class\n","            s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n","\n","          dets_to_sort = np.empty((0,6))\n","\n","          # NOTE: We send in detected object class too\n","          for x1,y1,x2,y2,conf,detclass in det.cpu().detach().numpy():\n","            dets_to_sort = np.vstack((dets_to_sort, np.array([x1, y1, x2, y2, conf, detclass])))\n","            \n","          if opt['track']:\n","            tracked_dets = sort_tracker.update(dets_to_sort, opt['unique-track-color'])\n","            tracks = sort_tracker.getTrackers()\n","\n","              # draw boxes for visualization\n","            if len(tracked_dets)>0:\n","                bbox_xyxy = tracked_dets[:,:4]\n","                identities = tracked_dets[:, 8]\n","                categories = tracked_dets[:, 4]\n","                confidences = None\n","                depths = compute_depth(colored_depth, bbox_xyxy)\n","                mask_bbox = find_contour_bbox(fr, threshold=mask_threshold)\n","                for t, track in enumerate(reversed(tracks)):\n","                  grad = 0\n","                  direc = None\n","                  grad, direc, pix_len = get_grad(track, img0, opt['thickness'], sep, rate, gap, line_len) # draw line w/ cal grad, pts\n","                  grads.append(grad * (180 / 3.14))\n","                  direcs.append(direc)\n","                  vel.append(depths[t if t < len(tracked_dets) else 0] * pix_len)\n","                #grad_history.append(grad)\n","                #vel_history.append(vel)\n","                #depth_history.append(depths)\n","          else:\n","            bbox_xyxy = dets_to_sort[:,:4]\n","            identities = None\n","            categories = dets_to_sort[:, 5]\n","            confidences = dets_to_sort[:, 4]\n","\n","          img0, now_status, logs = identify_pedestrian_states(img0, bbox_xyxy, identities=identities, \n","                                                        categories=categories, names=names, grads=grads,\n","                                                        depth=depths, mask=mask_bbox, direc=direcs, vel=vel,\n","                                                        undefined_threshold=0.1, grad_threshold=40, vel_threshold=900, \n","                                                        x_values=[300, 800], depth_threshold=[[130, 140],[70, 100],[120, 160]],\n","                                                        img_sz=[w, h])\n","          pre_status, dis_cnt = maintain_sign(img0, now_status, pre_status, dis_cnt, img_sz=[w, h])\n","          w_logs = logging_infos(logs, datas=w_logs)\n","      output_img = np.hstack((colored_depth, fr, img0))\n","      t2 = time_synchronized()\n","      cv2.imshow('output.png', output_img)\n","      print(\"inference time : \", '{:.2f}'.format(t2 - t1), 'ms')\n","      if cv2.waitKey(1) == 27: # ESC\n","        print(\"KEY EXIT\")\n","        break\n","    break\n","      \n","video.release()\n","cv2.destroyAllWindows()"],"metadata":{"id":"BanQfSyjWfLg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tkinter Custom"],"metadata":{"id":"9WKTg1JSNwRT"}},{"cell_type":"code","source":["# Webcam\n","video = cv2.VideoCapture(0)\n","\n","# Video information\n","fps = int(video.get(cv2.CAP_PROP_FPS))\n","w = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n","h = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","print(\"WEBCAM RESOLUTION = {} X {}\".format(w, h))\n","#nframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","# GradAcc Parameters\n","sep = 1 # Sep param \n","rate = 8 # Acc param\n","gap = 5 # Gap param\n","line_len = 50 # direction line length\n","\n","# Initializing SORT Class\n","sort_tracker = Sort(max_age=5, min_hits=2, iou_threshold=0.2) \n","\n","# ZoeDepth Model preparation\n","zoe = torch.hub.load(\".\", \"ZoeD_NK\", source=\"local\", pretrained=True)\n","zoe = zoe.to('cuda')\n","\n","# BSUVNet Initialization\n","# Start Video Loader\n","vid_loader = videoLoader(\n","    0, \n","    empty_bg=cfg.BSUVNet.emtpy_bg, \n","    empty_win_len=cfg.BSUVNet.empty_win_len,\n","    empty_bg_path=cfg.BSUVNet.empty_bg_path,\n","    recent_bg=cfg.BSUVNet.recent_bg,\n","    seg_network=cfg.BSUVNet.seg_network,\n","    transforms_pre=cfg.BSUVNet.transforms_pre,\n","    transforms_post=cfg.BSUVNet.transforms_post\n","    )\n","tensor_loader = iter(torch.utils.data.DataLoader(dataset=vid_loader, batch_size=1))\n","\n","# Load BSUV-Net\n","bsuvnet = torch.load(cfg.BSUVNet.model_path)\n","bsuvnet.cuda().eval()\n","\n","# Initialzing object for writing video output\n","#output = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'DIVX'),fps , (w,h))\n","#zoe_output = cv2.VideoWriter('zoe_output.mp4', cv2.VideoWriter_fourcc(*'DIVX'),fps , (w,h))\n","#mask_output = cv2.VideoWriter('mask_output.mp4', cv2.VideoWriter_fourcc(*'DIVX'),fps , (w,h))\n","torch.cuda.empty_cache()\n","\n","# parameter\n","pre_status = None\n","dis_cnt = 0\n","#mask_threshold = 10\n","\n","# logging\n","w_logs = np.array([])\n","#grad_history = []\n","#depth_history = []\n","#vel_history = []\n","# Initializing model and setting it for inference\n","with torch.no_grad():\n","  # YOLOv7 Preparation\n","  weights, imgsz = opt['weights'], opt['img-size']\n","  set_logging()\n","  device = select_device(opt['device'])\n","  half = device.type != 'cpu'\n","  model = attempt_load(weights, map_location=device)  # load FP32 model\n","  stride = int(model.stride.max())  # model stride\n","  imgsz = check_img_size(imgsz, s=stride)  # check img_size\n","  if half:\n","    model.half()\n","  names = model.module.names if hasattr(model, 'module') else model.names\n","  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n","  if device.type != 'cpu':\n","    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n","  classes = None\n","  if opt['classes']:\n","    classes = []\n","    for class_name in opt['classes']:\n","      classes.append(names.index(class_name))\n","  if classes:\n","    classes = [i for i in range(len(names)) if i not in classes]\n","#for j in tqdm(range(nframes)):\n","def show_frame():\n","  with torch.no_grad():\n","    global image_id, image_id2, image_id3, pre_status, dis_cnt, w_logs, depths, mask_bbox  # inform function to assign new value to global variable instead of local variable\n","    data = next(tensor_loader)\n","    t1 = time_synchronized()\n","    inp, img0 = data\n","    img0 = img0.squeeze().numpy()\n","    # Zoedepth write results\n","    zoe_img = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB) \n","    zoe_img = Image.fromarray(zoe_img)  # transform cv2 to pil\n","    depth_pil = zoe.infer_pil(zoe_img, output_type=\"tensor\")  # tensor image\n","    colored_depth = colorize(depth_pil)[:, :, :3] # Gray scale\n","    # BSUVNet write results\n","    bgs_pred = bsuvnet(inp.cuda().float()).cpu().numpy()[0, 0, :, :]\n","    fr = np.ones((h, w, 3)) * 0.5\n","    for ch in range(3):\n","      fr[:, :, ch] = bgs_pred\n","    fr = (fr * 255).astype(np.uint8) # Result [720, 1280, 3], all channel have the same value, gray scale\n","    # YOLO Prediction\n","    yolo_img = letterbox(img0, imgsz, stride=stride)[0]\n","    yolo_img = yolo_img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n","    yolo_img = np.ascontiguousarray(yolo_img)\n","    yolo_img = torch.from_numpy(yolo_img).to(device)\n","    yolo_img = yolo_img.half() if half else yolo_img.float()  # uint8 to fp16/32\n","    yolo_img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","    if yolo_img.ndimension() == 3:\n","      yolo_img = yolo_img.unsqueeze(0)\n","\n","    # YOLOv7 Inference\n","    pred = model(yolo_img, augment= False)[0]\n","    pred = non_max_suppression(pred, opt['conf-thres'], opt['iou-thres'], classes= classes, agnostic= False)\n","        \n","    #########\n","    grads = []\n","    depths = []\n","    direcs = []\n","    vel = []\n","    # YOLO Unpacking Predictions\n","    for i, det in enumerate(pred):\n","      #s = ''\n","      #s += '%gx%g ' % yolo_img.shape[2:]  # print string\n","      #gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n","      if len(det):\n","        det[:, :4] = scale_coords(yolo_img.shape[2:], det[:, :4], img0.shape).round()\n","\n","        #for c in det[:, -1].unique():\n","          #n = (det[:, -1] == c).sum()  # detections per class\n","          #s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n","\n","        dets_to_sort = np.empty((0,6))\n","\n","        # NOTE: We send in detected object class too\n","        for x1,y1,x2,y2,conf,detclass in det.cpu().detach().numpy():\n","          dets_to_sort = np.vstack((dets_to_sort, np.array([x1, y1, x2, y2, conf, detclass])))\n","            \n","        if opt['track']:\n","          tracked_dets = sort_tracker.update(dets_to_sort, opt['unique-track-color'])\n","          tracks = sort_tracker.getTrackers()\n","\n","          # draw boxes for visualization\n","          if len(tracked_dets)>0:\n","            bbox_xyxy = tracked_dets[:,:4]\n","            identities = tracked_dets[:, 8]\n","            categories = tracked_dets[:, 4]\n","            confidences = None\n","            depths = compute_depth(colored_depth, bbox_xyxy)\n","            mask_bbox = find_contour_bbox(fr, threshold=mask_threshold.get())\n","            for t, track in enumerate(reversed(tracks)):\n","              grad = 0\n","              direc = None\n","              grad, direc, pix_len = get_grad(track, img0, opt['thickness'], sep, rate, gap, line_len) # draw line w/ cal grad, pts\n","              grads.append(grad * (180 / 3.14))\n","              direcs.append(direc)\n","              vel.append(depths[t if t < len(tracked_dets) else 0] * pix_len)\n","            #grad_history.append(grad)\n","            #vel_history.append(vel)\n","            #depth_history.append(depths)\n","          else:\n","            bbox_xyxy = dets_to_sort[:,:4]\n","            identities = None\n","            categories = dets_to_sort[:, 5]\n","            confidences = dets_to_sort[:, 4]\n","\n","          img0, now_status, logs = identify_pedestrian_states(img0, bbox_xyxy, identities=identities, \n","                                                        categories=categories, names=names, grads=grads,\n","                                                        depth=depths, mask=mask_bbox, direc=direcs, vel=vel,\n","                                                        undefined_threshold=unknown_bbox_expansion_ratio.get(), grad_threshold=grad_threshold.get(), vel_threshold=vel_threshold.get(), \n","                                                        x_values=[x_values1.get(), x_values2.get()], depth_threshold=[[depth_threshold1_1.get(), depth_threshold1_2.get()],\n","                                                        [depth_threshold2_1.get(), depth_threshold2_2.get()],[depth_threshold3_1.get(), depth_threshold3_2.get()]],\n","                                                        img_sz=[w, h])\n","          pre_status, dis_cnt = maintain_sign(img0, now_status, pre_status, dis_cnt, img_sz=[w, h])\n","          w_logs = logging_infos(logs, datas=w_logs)\n","    cv2.line(img0, (x_values1.get(), 0), (x_values1.get(), h), (150, 200, 150), 2)\n","    cv2.line(img0, (x_values2.get(), 0), (x_values2.get(), h), (0, 200, 0), 2)\n","    #output_img = np.hstack((colored_depth, fr, img0))\n","    output_img = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n","    output_img = Image.fromarray(output_img)\n","    colored_depth = Image.fromarray(colored_depth)\n","    fr = Image.fromarray(fr)\n","\n","    # convert to Tkinter image\n","    photo = ImageTk.PhotoImage(image=output_img)\n","    photo2 = ImageTk.PhotoImage(image=colored_depth)\n","    photo3 = ImageTk.PhotoImage(image=fr)\n","       \n","    # solution for bug in `PhotoImage`\n","    canvas.photo = photo\n","    canvas2.photo = photo2\n","    canvas3.photo = photo3\n","\n","    # check if image already exists\n","    if image_id:       \n","      # replace image in PhotoImage on canvas\n","      canvas.itemconfig(image_id, image=photo)\n","    else:\n","      # create first image on canvas and keep its ID\n","      image_id = canvas.create_image((0, 0), image=photo, anchor='nw')\n","      # resize canvas\n","      canvas.configure(width=photo.width(), height=photo.height())\n","      # replace image in label\n","      #label.configure(image=photo)  \n","\n","    if image_id2:       \n","      # replace image in PhotoImage on canvas\n","      canvas2.itemconfig(image_id2, image=photo2)\n","    else:\n","      # create first image on canvas and keep its ID\n","      image_id2 = canvas2.create_image((0, 0), image=photo2, anchor='nw')\n","      # resize canvas\n","      canvas2.configure(width=photo2.width(), height=photo2.height())\n","      # replace image in label\n","      #label.configure(image=photo)\n","\n","    if image_id3:       \n","      # replace image in PhotoImage on canvas\n","      canvas3.itemconfig(image_id3, image=photo3)\n","    else:\n","      # create first image on canvas and keep its ID\n","      image_id3 = canvas3.create_image((0, 0), image=photo3, anchor='nw')\n","      # resize canvas\n","      canvas3.configure(width=photo3.width(), height=photo3.height())\n","      # replace image in label\n","      #label.configure(image=photo)\n","    # run again after 5ms\n","    t2 = time_synchronized()\n","    print(\"inference time : \", '{:.2f}'.format(t2 - t1 + 0.005), 's')\n","    window.after(5, show_frame)\n","    \n","# --- main ---\n","image_id = None # YOLO output\n","image_id2 = None # Depth output\n","image_id3 = None # BGS output\n","\n","window = tk.Tk()\n","font1 = font.Font(family='Arial', weight='bold', size='20')\n","font2 = font.Font(family='Arial', weight='bold', size='10')\n","window.title(\"DETECTION\")\n","window.geometry(\"{}x{}\".format(w + 400, h))\n","\n","# Set TkNotebook style\n","style = ttk.Style()\n","\n","style.theme_create( \"hsbtheme\", parent=\"alt\", settings={\n","        \"TNotebook\": {\"configure\": {\"tabmargins\": [2, 5, 2, 0], \"background\": 'lavender', \"bd\": 0}},\n","        \"TNotebook.Tab\": {\n","            \"configure\": {\"padding\": [15, 5], \"background\": 'light sky blue' , \"foreground\": \"white\", \"font\": font2, \"bordercolor\": 'lavender'},\n","            \"map\":       {\"background\": [(\"selected\", 'lavender')],\n","                          \"expand\": [(\"selected\", [1, 1, 1, 0])] } } } )\n","\n","style.theme_use(\"hsbtheme\")\n","\n","# create a Label to display frames\n","label = tk.Label(window, text='Pedestrian Detection', anchor='s', width=1920, height=1, background='lavender', fg='white', font=font1)\n","label.pack()  # to resize label when resize window\n","\n","frm1=tk.Frame(window, relief=\"solid\", background='lavender')\n","frm1.pack(side=\"left\", fill=\"both\", expand=True)\n","\n","frm2=tk.Frame(window, relief=\"solid\", background='lavender')\n","frm2.pack(side=\"right\", fill=\"both\", expand=True)\n","\n","frm1_1 = tk.Frame(frm1, relief=\"solid\", background='lavender')\n","frm1_1.pack(side=\"left\", fill=\"both\", expand=True)\n","\n","frm1_2 = tk.Frame(frm1, relief=\"solid\", background='lavender')\n","frm1_2.pack(side=\"right\", fill=\"both\", expand=True)\n","\n","# sliders\n","x_values1 = tk.Scale(frm1_1, label='x_values1', fg='white', from_=0, to=w, length=200, background='light sky blue', troughcolor='white', resolution=10, sliderrelief='flat', orient='horizontal', font=font2, relief='groove', bd=0)\n","x_values1.pack()\n","x_values1.set(300)\n","\n","x_values2 = tk.Scale(frm1_2, label='x_values2', fg='white', from_=0, to=w, length=200, background='light sky blue', troughcolor='white', resolution=10, sliderrelief='flat', orient='horizontal', font=font2, bd=0)\n","x_values2.pack()\n","x_values2.set(w if 800 > w  else 800)\n","\n","grad_threshold = tk.Scale(frm1_1, label='grad_threshold', fg='white', from_=0, to=90, length=200, background='light sky blue', troughcolor='white', resolution=5, sliderrelief='flat', orient='horizontal', font=font2, bd=0)\n","grad_threshold.pack()\n","grad_threshold.set(40)\n","\n","vel_threshold = tk.Scale(frm1_2, label='vel_threshold', fg='white', from_=0, to=2000, length=200, background='light sky blue', troughcolor='white', resolution=100, sliderrelief='flat', orient='horizontal', font=font2, bd=0)\n","vel_threshold.pack()\n","vel_threshold.set(900)\n","\n","mask_threshold = tk.Scale(frm1_1, label='mask_threshold', fg='white', from_=0, to=500, length=200, background='light sky blue', troughcolor='white', resolution=10, sliderrelief='flat', orient='horizontal', font=font2, bd=0)\n","mask_threshold.pack()\n","mask_threshold.set(10)\n","\n","unknown_bbox_expansion_ratio = tk.Scale(frm1_2, label='unknown_bbox_expansion_ratio', fg='white', from_=0, to=1, length=200, background='light sky blue', troughcolor='white', resolution=0.05, sliderrelief='flat', orient='horizontal', font=font2, bd=0)\n","unknown_bbox_expansion_ratio.pack()\n","unknown_bbox_expansion_ratio.set(0.1)\n","\n","depth_threshold1_1 = tk.Scale(frm1_1, label='depth_threshold1_1', fg='white', from_=0, to=255, length=200, background='light sky blue', troughcolor='white', resolution=5, sliderrelief='flat', orient='horizontal', font=font2, bd=0)\n","depth_threshold1_1.pack()\n","depth_threshold1_1.set(130)\n","\n","depth_threshold1_2 = tk.Scale(frm1_2, label='depth_threshold1_2', fg='white', from_=0, to=255, length=200, background='light sky blue', troughcolor='white', resolution=5, sliderrelief='flat', orient='horizontal', font=font2, bd=0)\n","depth_threshold1_2.pack()\n","depth_threshold1_2.set(140)\n","\n","depth_threshold2_1 = tk.Scale(frm1_1, label='depth_threshold2_1', fg='white', from_=0, to=255, length=200, background='light sky blue', troughcolor='white', resolution=5, sliderrelief='flat', orient='horizontal', font=font2, bd=0)\n","depth_threshold2_1.pack()\n","depth_threshold2_1.set(70)\n","\n","depth_threshold2_2 = tk.Scale(frm1_2, label='depth_threshold2_2', fg='white', from_=0, to=255, length=200, background='light sky blue', troughcolor='white', resolution=5, sliderrelief='flat', orient='horizontal', font=font2, bd=0)\n","depth_threshold2_2.pack()\n","depth_threshold2_2.set(100)\n","\n","depth_threshold3_1 = tk.Scale(frm1_1, label='depth_threshold3_1', fg='white', from_=0, to=255, length=200, background='light sky blue', troughcolor='white', resolution=5, sliderrelief='flat', orient='horizontal', font=font2, bd=0)\n","depth_threshold3_1.pack()\n","depth_threshold3_1.set(120)\n","\n","depth_threshold3_2 = tk.Scale(frm1_2, label='depth_threshold3_2', fg='white', from_=0, to=255, length=200, background='light sky blue', troughcolor='white', resolution=5, sliderrelief='flat', orient='horizontal', font=font2, bd=0)\n","depth_threshold3_2.pack()\n","depth_threshold3_2.set(160)\n","\n","# create notebook\n","notebook = ttk.Notebook(frm2, width=w, height=h)\n","notebook.pack()\n","\n","frame1=tk.Frame(window)\n","notebook.add(frame1, text=\"YOLO\")\n","\n","canvas = tk.Canvas(frame1, width=w, height=h, relief=\"solid\", bg='white')\n","canvas.pack(fill='both', expand=True)\n","\n","frame2=tk.Frame(window)\n","notebook.add(frame2, text=\"Depth\")\n","\n","canvas2 = tk.Canvas(frame2, width=w, height=h, relief=\"solid\", bg='white')\n","canvas2.pack(fill='both', expand=True)\n","\n","frame3=tk.Frame(window)\n","notebook.add(frame3, text=\"BGS\")\n","\n","canvas3 = tk.Canvas(frame3, width=w, height=h, relief=\"solid\", bg='white')\n","canvas3.pack(fill='both', expand=True)\n","\n","notebook.pack(expand=1, fill='both', padx=5, pady=5)\n","\n","# start function which shows frame\n","show_frame()\n","window.mainloop()\n","video.release()"],"metadata":{"id":"1nFt4ZLUNyyO"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["ZypET6s7zNii"],"provenance":[],"gpuType":"T4"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0af71121b4b94d6598fc7f2d0b88e1df":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_90ad7fa6cbe8412d8044dd14918624cf","IPY_MODEL_5e1527b55b75449aa295c5bf012e5c1c","IPY_MODEL_ec57b6ac1d8e40b8a887c9a1d5bb57e0"],"layout":"IPY_MODEL_0c638db7cae5467fbc723046cabad142"}},"90ad7fa6cbe8412d8044dd14918624cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24ed9e35c2e8494783e921791460a966","placeholder":"​","style":"IPY_MODEL_f98e6a29a4ad4785b064bf0603365a6f","value":"100%"}},"5e1527b55b75449aa295c5bf012e5c1c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4e9b5a90cd44e428359e0dde41093ac","max":348,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dccf740b346741d695153bd7471e9e45","value":348}},"ec57b6ac1d8e40b8a887c9a1d5bb57e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b677b069ac442da917ae2871690bcc6","placeholder":"​","style":"IPY_MODEL_3f39621951274964bb0c75ea47a09dfb","value":" 348/348 [17:09&lt;00:00,  2.88s/it]"}},"0c638db7cae5467fbc723046cabad142":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24ed9e35c2e8494783e921791460a966":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f98e6a29a4ad4785b064bf0603365a6f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4e9b5a90cd44e428359e0dde41093ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dccf740b346741d695153bd7471e9e45":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3b677b069ac442da917ae2871690bcc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f39621951274964bb0c75ea47a09dfb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}